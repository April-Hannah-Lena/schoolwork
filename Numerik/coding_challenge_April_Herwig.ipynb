{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Code Challenge - Praktikum Data Analytics"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import scipy as sp\r\n",
    "import sklearn as skl\r\n",
    "import nltk\r\n",
    "import re\r\n",
    "import string"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "data = pd.read_csv(\"account_turnovers.csv\", parse_dates=[\"entry_date\"], na_values='')\r\n",
    "detail_category = np.array(data[\"detail_category\"])\r\n",
    "data = data.drop(columns=[\r\n",
    "    \"detail_category\", \"account_id\", \"turnover_id\", \"user_id\", \"holder_name\", \"entry_date\"\r\n",
    "])\r\n",
    "# \"account_id\", \"turnover_id\", \"user_id\" sind eindeutig und können somit zu overfitting führen\r\n",
    "# \"holder_name\" *muss* nicht eindeutig sein, ist aber logischerweise so\r\n",
    "# \"entry_date\" hat nur 122 von 1575 Einträge gefüllt, wtf"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "umlauts = {'ü': 'ue', 'ä': 'ae', 'ö': 'oe', 'ß': 'ss'}\r\n",
    "stopwords_de = nltk.corpus.stopwords.words('german')\r\n",
    "\r\n",
    "def cleanup(in_str):\r\n",
    "    out_str = str(in_str).lower()\r\n",
    "    # reduziere Redundanz dass durch Großschreibung kommt\r\n",
    "\r\n",
    "    out_str = re.sub('bic ........', '', out_str)\r\n",
    "    out_str = re.sub('bic ...........', '', out_str)\r\n",
    "    # finde und entferne BIC-codes: immer 8- oder 11-stellig\r\n",
    "    \r\n",
    "    out_str = re.sub(\r\n",
    "        '[{}]'.format(re.escape(string.punctuation)),\r\n",
    "        ' ', out_str\r\n",
    "    ) # entferne unnötige Zeichen\r\n",
    "\r\n",
    "    out_str = re.sub('\\d', '', out_str)\r\n",
    "    # fast alle Zahlen im Verwendungszweck sind für Identifizierung, \r\n",
    "    # Potential für overfitting\r\n",
    "\r\n",
    "    for key, val in umlauts.items():\r\n",
    "        out_str = re.sub(key, val, out_str)\r\n",
    "    # reduziere Redundanz dass durch Ümläute kommt\r\n",
    "\r\n",
    "    out_str = re.sub(r\"((?<=^)|(?<= )).((?=$)|(?= ))\", '', out_str)\r\n",
    "    # nach Entfernen von Zahlen gibt es viele einstellige Wörter. \r\n",
    "    # Dies wird später zu overfitting führen nach dem encoding\r\n",
    "\r\n",
    "    out_str = out_str.split()\r\n",
    "    out_str = ' '.join([\r\n",
    "        token for token in out_str if not token in stopwords_de\r\n",
    "    ]) # normalisiere die Anzahl Leerzeichen und entferne Stopwörter im letzten Schritt\r\n",
    "    \r\n",
    "    return out_str"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "for column in (\"acct_name\", \"acct_type\", \"entry_text\", \"payee_payer_name\", \"paymt_purpose\"):\r\n",
    "    data[column] = data[column].apply(cleanup)\r\n",
    "# erste Runde von preprocessing auf die Spalten mit Strings"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "most_com = nltk.FreqDist( ' '.join(data[\"paymt_purpose\"]).split() ).most_common(25)\r\n",
    "most_com"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('svwz', 974),\n",
       " ('de', 537),\n",
       " ('eref', 461),\n",
       " ('mref', 447),\n",
       " ('miete', 422),\n",
       " ('dezzz', 408),\n",
       " ('cred', 396),\n",
       " ('end', 340),\n",
       " ('ref', 264),\n",
       " ('iban', 249),\n",
       " ('nr', 238),\n",
       " ('abwa', 238),\n",
       " ('bic', 217),\n",
       " ('tan', 188),\n",
       " ('datum', 179),\n",
       " ('to', 172),\n",
       " ('uhr', 163),\n",
       " ('kref', 153),\n",
       " ('danke', 141),\n",
       " ('purp', 122),\n",
       " ('id', 122),\n",
       " ('sepa', 110),\n",
       " ('rg', 100),\n",
       " ('karte', 98),\n",
       " ('eur', 94)]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Wie man sieht, sind die häfigsten 25 Wörter von der Spalte \"paymt_purpose\" meistens nicht nutzvoll. Nach 25 kommt \"basislastschrift\", was nutzvoll sein könnte. Deswegen werden wir diese 25 von der Spalte entfernen (bis auf ein Paar Ausnahmen)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "most_com.pop(4); most_com.pop(22)\r\n",
    "# wir nehmen die nutzvollen Wörter aus unserer Blacklist\r\n",
    "\r\n",
    "for word, freq in most_com:\r\n",
    "    stopwords_de.append(word)\r\n",
    "data[\"paymt_purpose\"] = data[\"paymt_purpose\"].apply(cleanup)\r\n",
    "# zweite Runde proprocessing, diesmal mit Blacklist\r\n",
    "\r\n",
    "data.to_csv(\"editeddata.csv\")\r\n",
    "# immer schön die proprocessed daten ausspucken"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "amount = np.array(data[\"amount\"]).reshape(-1, 1)\r\n",
    "gv_code = np.nan_to_num(data[\"gv_code\"]).reshape(-1, 1)\r\n",
    "# \"gv_code\" hat ein Paar unbenutzbare NaNs\r\n",
    "\r\n",
    "data_matrix = sp.sparse.csr_matrix(\r\n",
    "    np.hstack((\r\n",
    "        (amount - amount.mean()) / amount.std(),\r\n",
    "        (gv_code - gv_code.mean()) / gv_code.std()\r\n",
    "    ))\r\n",
    ") # normalisiere die float-Spalten\r\n",
    "\r\n",
    "for column in (\"acct_name\", \"acct_type\", \"entry_text\", \"payee_payer_name\", \"paymt_purpose\"):\r\n",
    "    data_matrix = sp.sparse.hstack([\r\n",
    "        data_matrix, \r\n",
    "        skl.feature_extraction.text.CountVectorizer().fit_transform(data[column])\r\n",
    "    ]) # encode die Wörter in den kategorischen String-Spalten"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "data_matrix = data_matrix.tocsr()\r\n",
    "test_data_matrix = data_matrix[1500:, :]\r\n",
    "data_matrix = data_matrix[:1500, :]\r\n",
    "test_detail_category = detail_category[1500:]\r\n",
    "detail_category = detail_category[:1500]\r\n",
    "# splite die Daten in Train- und Testset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "clf = skl.svm.SVC().fit(data_matrix, detail_category)\r\n",
    "# ganz normales SVM ohne Kernel weil ich keine Zeit mehr habe"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "test_detail_category_preds = clf.predict(test_data_matrix)\r\n",
    "(test_detail_category == test_detail_category_preds).sum() / test_detail_category.shape[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8666666666666667"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "86% ist gar ned mal so schlecht"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "aed11b59a6d24b307aec138dcbd86072676afa1d7eeadedf5a9c7f4d26e0dd03"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}